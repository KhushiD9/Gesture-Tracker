{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b6a226-788e-4215-b957-26f5d0e225bf",
   "metadata": {},
   "source": [
    "For training the model using a different dataset\n",
    "1. You need to make sure that the dataset is having test or train directories or not\n",
    "2. If not, divide it accordingly by 80% for training and 20% for testing.\n",
    "3. specify the dataset path accordingly\n",
    "4. In our case, the dataset was already divided into test and train directories.\n",
    "5. For custom input images, we have already made structured code, where you just need to specify the path.\n",
    "6. For input images, other than that of the dataset, use the CV code provided to change and then feed it into testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6701e119-9d1e-49d9-a481-6b0cea59efe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (0.2.9)\n",
      "Requirement already satisfied: packaging in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from kagglehub) (4.67.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
      "Found C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"anurags397/hindi-mnist-data\")\n",
    "print(\"Found\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40782a98-cc5f-46f3-b9c4-662fb7afbd3e",
   "metadata": {},
   "source": [
    "Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40459f4-7f28-490c-b63b-02b5ae14058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch scikit-learn numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886d3a6c-da6a-49c4-bd7c-2949dfe7f6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of dataset directory: ['DevanagariHandwrittenDigitDataset']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1'\n",
    "print(\"Contents of dataset directory:\", os.listdir(dataset_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f230ff-dbf9-4566-a8c6-8493095066bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of DevanagariHandwrittenDigitDataset: ['Test', 'Train']\n"
     ]
    }
   ],
   "source": [
    "subdirs = os.listdir(dataset_dir)\n",
    "\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(dataset_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        print(f\"Contents of {subdir}: {os.listdir(subdir_path)}\")\n",
    "    else:\n",
    "        print(f\"{subdir} is not a directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35eabe40-63c1-4889-bdf7-fd52c4eb0328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of classes in training data 10\n",
      "No. of images in each class in training data {'digit_0': 1700, 'digit_1': 1700, 'digit_2': 1700, 'digit_3': 1700, 'digit_4': 1700, 'digit_5': 1700, 'digit_6': 1700, 'digit_7': 1700, 'digit_8': 1700, 'digit_9': 1700}\n",
      "No. of classes in testing data 10\n",
      "No. of images in each class in testing data {'digit_0': 300, 'digit_1': 300, 'digit_2': 300, 'digit_3': 300, 'digit_4': 300, 'digit_5': 300, 'digit_6': 300, 'digit_7': 300, 'digit_8': 300, 'digit_9': 300}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir =  r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Train'\n",
    "test_dir =  r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Test'\n",
    "\n",
    "def count_images_and_classes(dir):\n",
    "    classes = os.listdir(dir)\n",
    "    class_count = {}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(dir, cls)\n",
    "        if os.path.isdir(class_path):\n",
    "            num_images = len(os.listdir(class_path))\n",
    "            class_count[cls] = num_images\n",
    "\n",
    "    return class_count\n",
    "\n",
    "train_count = count_images_and_classes(train_dir)\n",
    "test_count = count_images_and_classes(test_dir)\n",
    "\n",
    "print(\"No. of classes in training data\", len(train_count))\n",
    "print(\"No. of images in each class in training data\", train_count)\n",
    "print(\"No. of classes in testing data\", len(test_count))\n",
    "print(\"No. of images in each class in testing data\", test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ada2c6-b981-4ee0-a04d-e0fe8417af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (32, 32)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = os.path.join(test_dir, 'digit_0', '4574.png')\n",
    "\n",
    "img = Image.open(image_path)\n",
    "print(f\"Image size: {img.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239a8dfa-fd5f-4df3-a03a-3e80861b51da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: torch==2.4.1 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch==2.4.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch==2.4.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch==2.4.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from torch==2.4.1->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f99fa3-49ce-4693-87f1-f407b520b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  \n",
    "    transforms.Resize((32, 32)),                  \n",
    "    transforms.ToTensor(),                       \n",
    "    transforms.Normalize((0.5,), (0.5,))          \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Test', transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in dataset:\n",
    "        images.append(image.view(-1).numpy())  \n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "#here y_train is the ground truth of x_train \n",
    "#in the same way, y_test is the ground truth of x_test\n",
    "X_train, y_train = prepare_data(train_dataset)\n",
    "X_test, y_test = prepare_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bd47c-2455-44c6-9193-d71720ee42c5",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6505707b-f2a4-4dc2-8a9f-3467f21284b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (1.4.2)\n",
      "Training Accuracy: 99.3000%\n",
      "Testing Accuracy: 98.2667%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       300\n",
      "           1       0.99      0.99      0.99       300\n",
      "           2       0.94      0.98      0.96       300\n",
      "           3       0.99      0.94      0.96       300\n",
      "           4       0.99      1.00      1.00       300\n",
      "           5       0.99      0.97      0.98       300\n",
      "           6       1.00      0.98      0.99       300\n",
      "           7       0.99      0.98      0.98       300\n",
      "           8       0.99      1.00      1.00       300\n",
      "           9       0.99      0.99      0.99       300\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[299   0   0   0   0   0   0   1   0   0]\n",
      " [  0 298   0   0   0   1   0   0   1   0]\n",
      " [  1   2 293   2   0   1   0   0   0   1]\n",
      " [  0   1  14 283   0   1   0   0   0   1]\n",
      " [  0   0   0   0 299   0   0   0   0   1]\n",
      " [  0   0   5   1   2 292   0   0   0   0]\n",
      " [  1   0   1   0   0   0 295   2   1   0]\n",
      " [  5   0   0   0   0   1   0 293   1   0]\n",
      " [  0   0   0   0   0   0   0   0 300   0]\n",
      " [  1   0   0   1   0   0   1   1   0 296]]\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib\n",
    "import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#training the model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3) #k=3\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(knn_model, 'knn_model.pkl')\n",
    "\n",
    "y_train_pred = knn_model.predict(X_train)\n",
    "y_test_pred = knn_model.predict(X_test)\n",
    "\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.4f}%')\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Testing Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d658684-90b0-4805-b601-d78f15a07117",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "167ee91c-3edd-4a29-80dc-b46c55bcf20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0000%\n",
      "Testing Accuracy: 99.3000%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       300\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       0.98      0.99      0.98       300\n",
      "           3       0.99      0.99      0.99       300\n",
      "           4       1.00      1.00      1.00       300\n",
      "           5       0.99      0.99      0.99       300\n",
      "           6       1.00      0.99      0.99       300\n",
      "           7       0.99      0.99      0.99       300\n",
      "           8       0.99      1.00      1.00       300\n",
      "           9       1.00      0.99      0.99       300\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[299   0   0   0   0   0   0   1   0   0]\n",
      " [  0 299   0   0   0   0   0   0   1   0]\n",
      " [  0   0 296   2   0   1   0   0   0   1]\n",
      " [  0   0   4 296   0   0   0   0   0   0]\n",
      " [  0   0   0   0 299   1   0   0   0   0]\n",
      " [  0   0   2   0   1 296   1   0   0   0]\n",
      " [  0   1   0   0   0   0 298   1   0   0]\n",
      " [  0   0   0   0   0   2   0 298   0   0]\n",
      " [  0   0   0   0   0   0   0   0 300   0]\n",
      " [  0   0   1   0   0   0   0   0   1 298]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(svm_model, 'svm_model.pkl')\n",
    "\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.4f}%')\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Testing Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673cf9e-e342-44d2-b1d8-4cb008eaccd6",
   "metadata": {},
   "source": [
    "Convolutional Neural Network(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73052e3f-91e0-431d-8d31-bfb4fad95875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "        self.fc2 = nn.Linear(128, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) #32*16*16\n",
    "        x = self.pool(F.relu(self.conv2(x))) #64*8*8\n",
    "        x = self.pool(F.relu(self.conv3(x))) #128*4*4\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e45b7bc-397b-4583-9097-fcd5eeb0fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss:  0.3451\n",
      "Epoch 2/15, Train Loss:  0.0841\n",
      "Epoch 3/15, Train Loss:  0.0536\n",
      "Epoch 4/15, Train Loss:  0.0400\n",
      "Epoch 5/15, Train Loss:  0.0330\n",
      "Epoch 6/15, Train Loss:  0.0232\n",
      "Epoch 7/15, Train Loss:  0.0233\n",
      "Epoch 8/15, Train Loss:  0.0197\n",
      "Epoch 9/15, Train Loss:  0.0163\n",
      "Epoch 10/15, Train Loss:  0.0204\n",
      "Epoch 11/15, Train Loss:  0.0144\n",
      "Epoch 12/15, Train Loss:  0.0167\n",
      "Epoch 13/15, Train Loss:  0.0157\n",
      "Epoch 14/15, Train Loss:  0.0075\n",
      "Epoch 15/15, Train Loss:  0.0071\n",
      "Model saved as cnn_model.pth\n",
      "Training Accuracy: 99.9824%\n",
      "Testing Accuracy: 99.6000%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       300\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       0.98      1.00      0.99       300\n",
      "           3       1.00      0.99      0.99       300\n",
      "           4       1.00      0.99      1.00       300\n",
      "           5       0.99      0.99      0.99       300\n",
      "           6       1.00      1.00      1.00       300\n",
      "           7       1.00      1.00      1.00       300\n",
      "           8       1.00      1.00      1.00       300\n",
      "           9       0.99      1.00      1.00       300\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[300   0   0   0   0   0   0   0   0   0]\n",
      " [  0 299   0   0   0   0   0   0   1   0]\n",
      " [  0   1 299   0   0   0   0   0   0   0]\n",
      " [  0   0   2 297   0   0   0   0   0   1]\n",
      " [  0   0   0   0 298   2   0   0   0   0]\n",
      " [  0   0   3   0   0 296   1   0   0   0]\n",
      " [  0   0   0   0   0   0 300   0   0   0]\n",
      " [  0   0   0   0   0   0   0 299   0   1]\n",
      " [  0   0   0   0   0   0   0   0 300   0]\n",
      " [  0   0   0   0   0   0   0   0   0 300]]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = len(train_dataset.classes)  \n",
    "model = CNNModel(num_classes).to(device)\n",
    "\n",
    "#loss function as catergorial crossentropy and optimzer as adam also learning rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 32\n",
    "validation_steps = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    step = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "            step += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    if step % validation_steps == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_images, val_labels in test_loader:\n",
    "                    val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_images)\n",
    "                    val_loss += criterion(val_outputs, val_labels).item()\n",
    "                    val_steps += 1\n",
    "            model.train()\n",
    "            avg_val_loss = val_loss / val_steps\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_loader)}], \"\n",
    "                  f\"Train Loss: {running_loss/step:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/len(train_loader): .4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")\n",
    "print(\"Model saved as cnn_model.pth\")\n",
    "\n",
    "model.eval()\n",
    "def evaluate_model(loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "train_preds, train_labels = evaluate_model(train_loader)\n",
    "test_preds, test_labels = evaluate_model(test_loader)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a3e47-f298-43b8-84e5-5463ab62c8e2",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93fc8b0e-5f0b-4c1f-9b57-30c94e5cc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  \n",
    "    transforms.Resize((32, 32)),               \n",
    "    transforms.ToTensor(),                       \n",
    "    transforms.Normalize((0.5,), (0.5,))          \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Train',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42170686-38b1-4fe3-9948-bf600c0e003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     12\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     14\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)  \n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "model_path = 'vgg16_devanagari_mnist.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def evaluate_model(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "train_preds, train_labels = evaluate_model(train_loader, model, device)\n",
    "test_preds, test_labels = evaluate_model(test_loader, model, device)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c52fe7-e20d-42ea-adbb-d7adbdabc69c",
   "metadata": {},
   "source": [
    "GoogLeNet(Inception v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d21a3af5-97cb-4327-bd83-b235b8b4d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0636\n",
      "Epoch [2/10], Loss: 0.4255\n",
      "Epoch [3/10], Loss: 0.2966\n",
      "Epoch [4/10], Loss: 0.2415\n",
      "Epoch [5/10], Loss: 0.2092\n",
      "Epoch [6/10], Loss: 0.1804\n",
      "Epoch [7/10], Loss: 0.1655\n",
      "Epoch [8/10], Loss: 0.1534\n",
      "Epoch [9/10], Loss: 0.1423\n",
      "Epoch [10/10], Loss: 0.1324\n",
      "Model saved.\n",
      "Training Accuracy: 98.0941%\n",
      "Testing Accuracy: 97.9667%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       300\n",
      "           1       1.00      0.99      0.99       300\n",
      "           2       0.98      0.95      0.97       300\n",
      "           3       0.96      0.96      0.96       300\n",
      "           4       0.99      0.98      0.98       300\n",
      "           5       0.96      0.98      0.97       300\n",
      "           6       0.93      0.99      0.96       300\n",
      "           7       0.99      0.99      0.99       300\n",
      "           8       0.99      1.00      1.00       300\n",
      "           9       0.99      0.96      0.98       300\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[299   0   0   0   0   0   0   1   0   0]\n",
      " [  0 297   0   0   0   0   2   0   0   1]\n",
      " [  0   0 285   5   1   4   4   1   0   0]\n",
      " [  0   0   3 289   0   1   6   0   0   1]\n",
      " [  0   0   0   0 294   5   1   0   0   0]\n",
      " [  0   0   0   1   0 295   3   0   1   0]\n",
      " [  0   0   0   3   0   0 296   1   0   0]\n",
      " [  0   0   0   0   0   1   3 296   0   0]\n",
      " [  0   0   0   0   0   1   0   0 299   0]\n",
      " [  0   0   2   2   2   1   2   1   1 289]]\n"
     ]
    }
   ],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1),  \n",
    "#     transforms.Resize((224, 224)),               \n",
    "#     transforms.ToTensor(),                       \n",
    "#     transforms.Normalize((0.5,), (0.5,))          \n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.Resize((224, 224)),                \n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Normalize((0.5,) * 3, (0.5,) * 3)  \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Train',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "model.conv2d_1a = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# nn.init.kaiming_normal_(model.conv2d_1a.weight, mode='fan_out', nonlinearity='relu')\n",
    "# model.transform_input = False\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "    \n",
    "torch.save(model.state_dict(), 'googlenet_devanagari_mnist.pth')\n",
    "print(f\"Model saved.\")\n",
    "\n",
    "train_preds, train_labels = evaluate_model(train_loader, model, device) \n",
    "test_preds, test_labels = evaluate_model(test_loader, model, device) \n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0d70a-8a12-41ab-8747-a38311eb5a61",
   "metadata": {},
   "source": [
    "ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204df7c0-d9e0-4a18-8442-28e0a7226d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8627\n",
      "Epoch [2/10], Loss: 0.3085\n",
      "Epoch [3/10], Loss: 0.2207\n",
      "Epoch [4/10], Loss: 0.1766\n",
      "Epoch [5/10], Loss: 0.1489\n",
      "Epoch [6/10], Loss: 0.1327\n",
      "Epoch [7/10], Loss: 0.1151\n",
      "Epoch [8/10], Loss: 0.1061\n",
      "Epoch [9/10], Loss: 0.0969\n",
      "Epoch [10/10], Loss: 0.0894\n",
      "Model saved.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50_devanagari_mnist.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m train_preds, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m(train_loader, model, device)\n\u001b[0;32m     60\u001b[0m test_preds, test_labels \u001b[38;5;241m=\u001b[39m evaluate_model(test_loader, model, device)  \n\u001b[0;32m     62\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(train_labels, train_preds)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.Resize((224, 224)),                \n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Normalize((0.5,) * 3, (0.5,) * 3)  \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Train',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\khush\\.cache\\kagglehub\\datasets\\anurags397\\hindi-mnist-data\\versions\\1\\DevanagariHandwrittenDigitDataset\\Test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader: \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'resnet50_devanagari_mnist.pth')\n",
    "print(f\"Model saved.\")\n",
    "\n",
    "train_preds, train_labels = evaluate_model(train_loader, model, device)\n",
    "test_preds, test_labels = evaluate_model(test_loader, model, device)  \n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0865dc58-d474-4e06-ad0f-faf594a9866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_26644\\1087068099.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnet50_devanagari_mnist.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.4176%\n",
      "Testing Accuracy: 97.2667%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       300\n",
      "           1       0.98      0.99      0.99       300\n",
      "           2       0.93      0.97      0.95       300\n",
      "           3       0.99      0.93      0.96       300\n",
      "           4       0.98      0.98      0.98       300\n",
      "           5       0.99      0.97      0.98       300\n",
      "           6       0.95      0.93      0.94       300\n",
      "           7       0.96      0.99      0.98       300\n",
      "           8       0.97      1.00      0.99       300\n",
      "           9       0.97      0.97      0.97       300\n",
      "\n",
      "    accuracy                           0.97      3000\n",
      "   macro avg       0.97      0.97      0.97      3000\n",
      "weighted avg       0.97      0.97      0.97      3000\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[296   0   0   0   0   0   0   3   1   0]\n",
      " [  0 297   0   0   0   0   1   0   1   1]\n",
      " [  0   0 292   3   2   2   0   0   0   1]\n",
      " [  0   0  11 280   0   0   7   0   0   2]\n",
      " [  0   1   1   0 295   0   1   1   0   1]\n",
      " [  0   2   0   1   2 291   2   1   1   0]\n",
      " [  0   1   5   0   0   1 280   5   4   4]\n",
      " [  0   0   1   0   0   0   2 296   1   0]\n",
      " [  0   0   0   0   0   0   0   0 300   0]\n",
      " [  0   2   4   0   1   0   1   1   0 291]]\n"
     ]
    }
   ],
   "source": [
    "# model = models.resnet50(pretrained=False)\n",
    "# num_classes = 10\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load('resnet50_devanagari_mnist.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('resnet50_devanagari_mnist.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "def evaluate_model(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "train_preds, train_labels = evaluate_model(train_loader, model, device)\n",
    "test_preds, test_labels = evaluate_model(test_loader, model, device)  \n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc890212-f5a5-4c9f-82d4-2cb88886e7c5",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edb8eb7f-9ce6-4fea-a944-da9fd7824ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_8932\\3924069665.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    model = CNNModel(num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)) \n",
    "    model.to(device).eval() \n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1), \n",
    "        transforms.Resize((32, 32)),  \n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('L') \n",
    "    input_tensor = transform(image).unsqueeze(0) \n",
    "    return input_tensor.to(device)\n",
    "\n",
    "\n",
    "def predict(model, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted_class = torch.max(outputs, 1)   \n",
    "        return predicted_class.item()  \n",
    "\n",
    "# # predicted_label = predict(model, input_tensor)\n",
    "# print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "# Image path and model path\n",
    "image_path = r'C:\\Users\\khush\\Desktop\\e.jpg' \n",
    "model_path = r'C:\\Users\\khush\\Downloads\\cnn_model.pth' \n",
    "\n",
    "\n",
    "num_classes = 10  \n",
    "input_tensor = preprocess_image(image_path)\n",
    "model = load_model(model_path, num_classes)\n",
    "predicted_label = predict(model, input_tensor)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9da827c7-ea84-4da9-b244-76400a1fce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_8932\\636837815.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    model = CNNModel(num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)) \n",
    "    model.to(device).eval() \n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1), \n",
    "        transforms.Resize((32, 32)),  \n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('L') \n",
    "    input_tensor = transform(image).unsqueeze(0) \n",
    "    return input_tensor.to(device)\n",
    "\n",
    "\n",
    "def predict(model, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted_class = torch.max(outputs, 1)   \n",
    "        return predicted_class.item()  \n",
    "\n",
    "# # predicted_label = predict(model, input_tensor)\n",
    "# print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "# Image path and model path\n",
    "image_path = r\"C:\\Users\\khush\\Desktop\\f.jpg\" \n",
    "model_path = r'C:\\Users\\khush\\Downloads\\cnn_model.pth' \n",
    "\n",
    "\n",
    "num_classes = 10  \n",
    "input_tensor = preprocess_image(image_path)\n",
    "model = load_model(model_path, num_classes)\n",
    "predicted_label = predict(model, input_tensor)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "057a779c-ad33-4857-bfee-3b92f49c69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label (SVM): 0\n"
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "import numpy as np\n",
    "\n",
    "# Load SVM model function\n",
    "def load_svm_model(model_path):\n",
    "    return joblib.load(model_path) \n",
    "\n",
    "def preprocess_image_for_svm(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1), \n",
    "        transforms.Resize((32, 32)),  \n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('L')  \n",
    "    input_tensor = transform(image).unsqueeze(0)  \n",
    "    input_vector = input_tensor.view(-1).numpy()  \n",
    "    return input_vector\n",
    "\n",
    "def predict_svm(model, input_vector):\n",
    "    predicted_class = model.predict([input_vector]) \n",
    "    return predicted_class[0]  \n",
    "\n",
    "# Image path and model path\n",
    "image_path = r'C:\\Users\\khush\\Desktop\\b.jpg'  \n",
    "svm_model_path = r'C:\\Users\\khush\\Downloads\\knn_model.pkl' ##can also use this for knn\n",
    "svm_model = load_svm_model(svm_model_path)\n",
    "input_vector = preprocess_image_for_svm(image_path)\n",
    "predicted_label_svm = predict_svm(svm_model, input_vector)\n",
    "print(f\"Predicted Label (SVM): {predicted_label_svm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d340b1df-1271-4241-b858-f9b7aaa4980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_8932\\4294212532.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path, device):\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)  \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "    return model\n",
    "\n",
    "def predict(model, input_tensor, device):\n",
    "    input_tensor = input_tensor.to(device)  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)  \n",
    "        _, predicted_class = torch.max(outputs, 1)  \n",
    "        return predicted_class.item() \n",
    "        \n",
    "image_path = r'C:\\Users\\khush\\Desktop\\a.jpg' \n",
    "model_path = 'vgg16_devanagari_mnist.pth'  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_model(model_path, device)\n",
    "input_tensor = preprocess_image(image_path)\n",
    "predicted_label = predict(model, input_tensor, device)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "731531fd-76d4-4fa7-a735-dfea9abfc170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages\\torchvision\\models\\googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_8932\\3876959849.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    model = models.googlenet(pretrained=False, aux_logits=False)\n",
    "    model.conv2d_1a = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  \n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))  \n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, input_tensor, device):\n",
    "    input_tensor = input_tensor.to(device)  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)  \n",
    "        _, predicted_class = torch.max(outputs, 1)  \n",
    "        return predicted_class.item() -2\n",
    "        \n",
    "def preprocess_image(image_path):\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  \n",
    "        transforms.Resize((224, 224)),              \n",
    "        transforms.ToTensor(),                       \n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  \n",
    "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)  \n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('L') \n",
    "    return transform(image).unsqueeze(0) \n",
    "\n",
    "\n",
    "image_path = r'C:\\Users\\khush\\Desktop\\a.jpg'  \n",
    "model_path = 'googlenet_devanagari_mnist.pth'  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = load_model(model_path, device)\n",
    "input_tensor = preprocess_image(image_path)\n",
    "predicted_label = predict(model, input_tensor, device)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8702a9-200a-4b89-a3ad-ebc345de72e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 32x32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "img_test = r'C:\\Users\\khush\\Desktop\\a.jpg'\n",
    "with Image.open(image_path) as img:\n",
    "    width, height = img.size\n",
    "    print(f\"Image size: {width}x{height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2be2f94-aafc-4086-8431-8cdc0fc498bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_8932\\2629239909.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    model = CNNModel(num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)) \n",
    "    model.to(device).eval() \n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1), \n",
    "        transforms.Resize((32, 32)),  \n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('L') \n",
    "    input_tensor = transform(image).unsqueeze(0) \n",
    "    return input_tensor.to(device)\n",
    "\n",
    "\n",
    "def predict(model, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted_class = torch.max(outputs, 1)   \n",
    "        return predicted_class.item()  \n",
    "\n",
    "# # predicted_label = predict(model, input_tensor)\n",
    "# print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "# Image path and model path\n",
    "image_path = r\"C:\\Users\\khush\\Desktop\\k.jpg\"  \n",
    "model_path = r'C:\\Users\\khush\\Downloads\\cnn_model.pth' \n",
    "\n",
    "\n",
    "num_classes = 10  \n",
    "input_tensor = preprocess_image(image_path)\n",
    "model = load_model(model_path, num_classes)\n",
    "predicted_label = predict(model, input_tensor)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05c41769-25a1-42ea-8136-e7264ab15dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\khush\\anaconda3\\envs\\dip\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Processed image saved to: C:\\Users\\khush\\Desktop\\d.png\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image_path = r'C:\\Users\\khush\\Desktop\\a.jpg'\n",
    "original_image = cv2.imread(image_path)\n",
    "\n",
    "gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, binary_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "digit_cropped = binary_image[y:y+h, x:x+w]\n",
    "max_side = max(digit_cropped.shape)\n",
    "padded_digit = cv2.copyMakeBorder(\n",
    "    digit_cropped,\n",
    "    top=(max_side - digit_cropped.shape[0]) // 2,\n",
    "    bottom=(max_side - digit_cropped.shape[0]) - (max_side - digit_cropped.shape[0]) // 2,\n",
    "    left=(max_side - digit_cropped.shape[1]) // 2,\n",
    "    right=(max_side - digit_cropped.shape[1]) - (max_side - digit_cropped.shape[1]) // 2,\n",
    "    borderType=cv2.BORDER_CONSTANT,\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "digit_resized = cv2.resize(padded_digit, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "final_image = 255 - digit_resized\n",
    "\n",
    "# Save the processed image\n",
    "output_path = r'C:\\Users\\khush\\Desktop\\d.png'\n",
    "cv2.imwrite(output_path, final_image)\n",
    "\n",
    "print(f\"Processed image saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce99c5-5321-4240-bb3a-2c153e224d35",
   "metadata": {},
   "source": [
    "CV code for changing the custom input image into the required image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4922bb8-85b2-4a4b-9672-72cf0a25037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image saved to C:\\Users\\khush\\Desktop\\processed.jpgprocessed_image.jpg\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_digit_to_white(image_path, save_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    inverted_image = cv2.bitwise_not(binary_image)\n",
    "    save_file = save_path + 'processed_image.jpg'  \n",
    "    cv2.imwrite(save_file, inverted_image)\n",
    "    print(f\"Processed image saved to {save_file}\")\n",
    "    plt.imshow(inverted_image, cmap='gray')\n",
    "    plt.title(\"Digit White, Background Black\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_path = r\"C:\\Users\\khush\\Desktop\\h.jpg\" \n",
    "save_path = r\"C:\\Users\\khush\\Desktop\\processed.jpg\"\n",
    "convert_digit_to_white(image_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf47706-329f-4692-8a07-9c7c5aea6bb4",
   "metadata": {},
   "source": [
    "For training the model using a different dataset\n",
    "1. You need to make sure that the dataset is having test or train directories or not\n",
    "2. If not, divide it accordingly by 80% for training and 20% for testing.\n",
    "3. specify the dataset path accordingly\n",
    "4. In our case, the dataset was already divided into test and train directories.\n",
    "5. For custom input images, we have already made structured code, where you just need to specify the path.\n",
    "6. For input images, other than that of the dataset, use the CV code provided to change and then feed it into testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6114e7-5820-40c9-9371-56b96fa2bd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
